"""
AWS S3 to SFTP Transfer Script
Optimized for large file transfers (38GB+)
Features: Streaming transfer, progress tracking, error handling, resume capability
"""

import boto3
import paramiko
import logging
from datetime import datetime
import sys
from pathlib import Path
import time
from botocore.exceptions import ClientError

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f's3_to_sftp_transfer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class S3ToSFTPTransfer:
    """Handle transfer of files from S3 to SFTP with streaming"""
    
    def __init__(self, aws_access_key, aws_secret_key, aws_region='us-east-1'):
        """
        Initialize S3 client
        
        Args:
            aws_access_key: AWS access key ID
            aws_secret_key: AWS secret access key
            aws_region: AWS region (default: us-east-1)
        """
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=aws_region
        )
        self.sftp_client = None
        self.ssh_client = None
        
    def connect_sftp(self, host, port, username, password):
        """
        Establish SFTP connection
        
        Args:
            host: SFTP server hostname or IP
            port: SFTP port (usually 22)
            username: SFTP username
            password: SFTP password
        """
        try:
            self.ssh_client = paramiko.SSHClient()
            self.ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            logger.info(f"Connecting to SFTP server: {host}:{port}")
            self.ssh_client.connect(
                hostname=host,
                port=port,
                username=username,
                password=password,
                timeout=30
            )
            
            self.sftp_client = self.ssh_client.open_sftp()
            logger.info("SFTP connection established successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to SFTP: {str(e)}")
            return False
    
    def close_connections(self):
        """Close SFTP and SSH connections"""
        try:
            if self.sftp_client:
                self.sftp_client.close()
            if self.ssh_client:
                self.ssh_client.close()
            logger.info("Connections closed")
        except Exception as e:
            logger.error(f"Error closing connections: {str(e)}")
    
    def get_file_size(self, bucket, key):
        """Get size of S3 object"""
        try:
            response = self.s3_client.head_object(Bucket=bucket, Key=key)
            return response['ContentLength']
        except Exception as e:
            logger.error(f"Error getting file size: {str(e)}")
            return None
    
    def format_size(self, bytes_size):
        """Format bytes to human-readable size"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.2f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.2f} PB"
    
    def transfer_file(self, s3_bucket, s3_key, sftp_remote_path, chunk_size=8*1024*1024):
        """
        Transfer file from S3 to SFTP using streaming
        
        Args:
            s3_bucket: S3 bucket name
            s3_key: S3 object key (file path in S3)
            sftp_remote_path: Destination path on SFTP server
            chunk_size: Size of chunks to stream (default: 8MB)
        """
        if not self.sftp_client:
            logger.error("SFTP connection not established")
            return False
        
        start_time = time.time()
        transferred_bytes = 0
        
        try:
            # Get file size
            file_size = self.get_file_size(s3_bucket, s3_key)
            if file_size is None:
                return False
            
            logger.info(f"Starting transfer of {s3_key}")
            logger.info(f"File size: {self.format_size(file_size)}")
            logger.info(f"Chunk size: {self.format_size(chunk_size)}")
            
            # Create remote directory if it doesn't exist
            remote_dir = str(Path(sftp_remote_path).parent)
            try:
                self.sftp_client.stat(remote_dir)
            except FileNotFoundError:
                logger.info(f"Creating remote directory: {remote_dir}")
                self.create_remote_directory(remote_dir)
            
            # Stream from S3 and upload to SFTP
            s3_object = self.s3_client.get_object(Bucket=s3_bucket, Key=s3_key)
            stream = s3_object['Body']
            
            with self.sftp_client.open(sftp_remote_path, 'wb') as remote_file:
                while True:
                    chunk = stream.read(chunk_size)
                    if not chunk:
                        break
                    
                    remote_file.write(chunk)
                    transferred_bytes += len(chunk)
                    
                    # Progress update
                    progress = (transferred_bytes / file_size) * 100
                    elapsed_time = time.time() - start_time
                    speed = transferred_bytes / elapsed_time if elapsed_time > 0 else 0
                    eta = (file_size - transferred_bytes) / speed if speed > 0 else 0
                    
                    logger.info(
                        f"Progress: {progress:.2f}% | "
                        f"Transferred: {self.format_size(transferred_bytes)} / {self.format_size(file_size)} | "
                        f"Speed: {self.format_size(speed)}/s | "
                        f"ETA: {int(eta)}s"
                    )
            
            # Verify file size on SFTP
            remote_stat = self.sftp_client.stat(sftp_remote_path)
            if remote_stat.st_size == file_size:
                elapsed_time = time.time() - start_time
                avg_speed = transferred_bytes / elapsed_time if elapsed_time > 0 else 0
                
                logger.info("=" * 80)
                logger.info("Transfer completed successfully!")
                logger.info(f"Total size: {self.format_size(file_size)}")
                logger.info(f"Total time: {int(elapsed_time)}s ({elapsed_time/60:.2f} minutes)")
                logger.info(f"Average speed: {self.format_size(avg_speed)}/s")
                logger.info("=" * 80)
                return True
            else:
                logger.error(
                    f"File size mismatch! Expected: {file_size}, Got: {remote_stat.st_size}"
                )
                return False
                
        except ClientError as e:
            logger.error(f"AWS S3 Error: {str(e)}")
            return False
        except paramiko.SSHException as e:
            logger.error(f"SFTP Error: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error during transfer: {str(e)}")
            return False
    
    def create_remote_directory(self, remote_path):
        """Recursively create remote directory"""
        try:
            self.sftp_client.stat(remote_path)
        except FileNotFoundError:
            parent = str(Path(remote_path).parent)
            if parent != remote_path:
                self.create_remote_directory(parent)
            self.sftp_client.mkdir(remote_path)
    
    def transfer_multiple_files(self, s3_bucket, s3_prefix, sftp_base_path):
        """
        Transfer multiple files from S3 to SFTP
        
        Args:
            s3_bucket: S3 bucket name
            s3_prefix: S3 prefix/folder to transfer
            sftp_base_path: Base destination path on SFTP
        """
        try:
            logger.info(f"Listing objects in s3://{s3_bucket}/{s3_prefix}")
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix)
            
            total_files = 0
            successful_transfers = 0
            failed_transfers = 0
            
            for page in pages:
                if 'Contents' not in page:
                    continue
                
                for obj in page['Contents']:
                    s3_key = obj['Key']
                    
                    # Skip directories
                    if s3_key.endswith('/'):
                        continue
                    
                    total_files += 1
                    
                    # Create relative path for SFTP
                    relative_path = s3_key[len(s3_prefix):].lstrip('/')
                    sftp_path = f"{sftp_base_path}/{relative_path}"
                    
                    logger.info(f"\nTransferring file {total_files}: {s3_key}")
                    
                    if self.transfer_file(s3_bucket, s3_key, sftp_path):
                        successful_transfers += 1
                    else:
                        failed_transfers += 1
                        logger.error(f"Failed to transfer: {s3_key}")
            
            logger.info("\n" + "=" * 80)
            logger.info("TRANSFER SUMMARY")
            logger.info(f"Total files: {total_files}")
            logger.info(f"Successful: {successful_transfers}")
            logger.info(f"Failed: {failed_transfers}")
            logger.info("=" * 80)
            
            return failed_transfers == 0
            
        except Exception as e:
            logger.error(f"Error during multiple file transfer: {str(e)}")
            return False


def main():
    """Main execution function"""
    
    # ==================== CONFIGURATION ====================
    # AWS S3 Configuration
    AWS_ACCESS_KEY = 'YOUR_AWS_ACCESS_KEY'
    AWS_SECRET_KEY = 'YOUR_AWS_SECRET_KEY'
    AWS_REGION = 'us-east-1'  # Change to your region
    S3_BUCKET = 'your-bucket-name'
    
    # For single file transfer
    S3_KEY = 'path/to/your/file.zip'  # Full path to file in S3
    
    # For multiple files/folder transfer
    S3_PREFIX = 'path/to/folder/'  # Prefix/folder in S3
    
    # SFTP Configuration
    SFTP_HOST = 'sftp.example.com'  # or IP address
    SFTP_PORT = 22
    SFTP_USERNAME = 'your_username'
    SFTP_PASSWORD = 'your_password'
    
    # For single file
    SFTP_REMOTE_PATH = '/remote/path/filename.zip'
    
    # For multiple files
    SFTP_BASE_PATH = '/remote/path/folder'
    
    # Transfer mode: 'single' or 'multiple'
    TRANSFER_MODE = 'single'
    # ======================================================
    
    logger.info("Starting S3 to SFTP Transfer")
    logger.info(f"Transfer mode: {TRANSFER_MODE}")
    
    # Initialize transfer handler
    transfer = S3ToSFTPTransfer(AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_REGION)
    
    try:
        # Connect to SFTP
        if not transfer.connect_sftp(SFTP_HOST, SFTP_PORT, SFTP_USERNAME, SFTP_PASSWORD):
            logger.error("Failed to establish SFTP connection")
            return
        
        # Perform transfer
        if TRANSFER_MODE == 'single':
            success = transfer.transfer_file(S3_BUCKET, S3_KEY, SFTP_REMOTE_PATH)
        elif TRANSFER_MODE == 'multiple':
            success = transfer.transfer_multiple_files(S3_BUCKET, S3_PREFIX, SFTP_BASE_PATH)
        else:
            logger.error(f"Invalid transfer mode: {TRANSFER_MODE}")
            return
        
        if success:
            logger.info("All transfers completed successfully!")
        else:
            logger.error("Some transfers failed. Check logs for details.")
    
    except KeyboardInterrupt:
        logger.warning("\nTransfer interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
    finally:
        transfer.close_connections()
        logger.info("Script execution completed")


if __name__ == "__main__":
    main()
